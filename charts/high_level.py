import streamlit as st
import pandas as pd
import plotly.express as px
import math
import hashlib
import time
from services.db import get_db

# ==================== æ•°æ®ç‰ˆæœ¬æ§åˆ¶ ====================
def get_data_version():
    """è·å–æ•°æ®ç‰ˆæœ¬ï¼Œç”¨äºå¼ºåˆ¶åˆ·æ–°ç¼“å­˜"""
    return st.session_state.get('data_version', 0)


def increment_data_version():
    """å¢åŠ æ•°æ®ç‰ˆæœ¬å·ï¼Œå¼ºåˆ¶åˆ·æ–°æ‰€æœ‰ç¼“å­˜"""
    current = st.session_state.get('data_version', 0)
    st.session_state.data_version = current + 1


def show_refresh_indicator():
    """æ˜¾ç¤ºåˆ·æ–°æŒ‡ç¤ºå™¨"""
    st.markdown('<div class="refresh-indicator" id="refreshIndicator"></div>', unsafe_allow_html=True)
    # 0.5ç§’åç§»é™¤æŒ‡ç¤ºå™¨
    st.markdown("""
    <script>
    setTimeout(function() {
        var indicator = document.getElementById('refreshIndicator');
        if (indicator) indicator.remove();
    }, 500);
    </script>
    """, unsafe_allow_html=True)


def clear_all_cache():
    """æ¸…é™¤æ‰€æœ‰ç¼“å­˜ - å…¨å±€å‡½æ•°ï¼Œç”¨äºæ‰€æœ‰æ¨¡å—"""
    # æ¸…é™¤session stateä¸­çš„ç¼“å­˜æ•°æ®
    keys_to_clear = [
        'precomputed_data', 'data_loaded',
        'hl_time', 'hl_data', 'hl_cats',
        'last_data_hash', 'cached_filtered_data'
    ]

    for key in list(st.session_state.keys()):
        if any(cache_key in key for cache_key in keys_to_clear):
            del st.session_state[key]

    # æ¸…é™¤streamlitç¼“å­˜
    try:
        get_high_level_data.clear()
        _prepare_inventory_grouped.clear()
        compute_filtered_data.clear()
    except:
        pass

    # å¢åŠ æ•°æ®ç‰ˆæœ¬å·
    increment_data_version()

    # æ˜¾ç¤ºåˆ·æ–°æŒ‡ç¤ºå™¨
    show_refresh_indicator()


# ==================== æ•°æ®å“ˆå¸Œæ£€æµ‹ ====================
def get_data_hash(tx, mem, inv):
    """ç”Ÿæˆæ•°æ®å“ˆå¸Œæ¥æ£€æµ‹æ•°æ®å˜åŒ–"""
    hash_parts = []

    # å¯¹æ¯ä¸ªæ•°æ®æ¡†ç”Ÿæˆå“ˆå¸Œ
    for df, name in [(tx, 'tx'), (mem, 'mem'), (inv, 'inv')]:
        if df is not None and not df.empty:
            # ä½¿ç”¨æ•°æ®å½¢çŠ¶å’Œå†…å®¹çš„å“ˆå¸Œ
            try:
                shape_hash = hash((df.shape[0], df.shape[1]))
                content_hash = pd.util.hash_pandas_object(df).sum()
                hash_parts.append(f"{name}_{shape_hash}_{content_hash}")
            except:
                hash_parts.append(f"{name}_error")
        else:
            hash_parts.append(f"{name}_empty")

    combined_hash = "_".join(hash_parts)
    return hashlib.md5(combined_hash.encode()).hexdigest()


# ==================== åŸæœ‰å·¥å…·å‡½æ•° ====================
def _safe_sum(df, col):
    if df is None or df.empty or col not in df.columns:
        return 0.0
    s = df[col]
    if pd.api.types.is_numeric_dtype(s):
        return float(pd.to_numeric(s, errors="coerce").sum(skipna=True))
    s = (
        s.astype(str)
        .str.replace(r"[^0-9\.\-]", "", regex=True)
        .replace("", pd.NA)
    )
    return float(pd.to_numeric(s, errors="coerce").sum(skipna=True) or 0.0)


def proper_round(x):
    """æ ‡å‡†çš„å››èˆäº”å…¥æ–¹æ³•ï¼Œ0.5æ€»æ˜¯å‘ä¸Šèˆå…¥"""
    if pd.isna(x):
        return x
    return math.floor(x + 0.5)


def persisting_multiselect(label, options, key, default=None):
    """
    ä¸€ä¸ªæŒä¹…åŒ–çš„ multiselect æ§ä»¶ï¼š
    - ç¬¬ä¸€æ¬¡åˆ›å»ºæ—¶ä¼šç”¨ default åˆå§‹åŒ–ï¼›
    - åç»­è¿è¡Œæ—¶å¦‚æœ session_state ä¸­å·²æœ‰å€¼ï¼Œåˆ™ä¸å†ä¼  defaultï¼ˆé˜²æ­¢å†²çªè­¦å‘Šï¼‰ã€‚
    """

    # å¦‚æœ Session State é‡Œå·²ç»å­˜åœ¨å€¼ï¼Œåˆ™ç›´æ¥è¿”å›æ§ä»¶ï¼Œä¸å†ä¼  defaultï¼Œé¿å…è­¦å‘Š
    if key in st.session_state:
        return st.multiselect(label, options, key=key)

    # å¦‚æœè¿˜æ²¡æœ‰åˆå§‹åŒ–ï¼Œå…ˆå†™å…¥é»˜è®¤å€¼
    init_value = default or []
    st.session_state[key] = init_value

    # ç¬¬ä¸€æ¬¡åˆ›å»ºæ§ä»¶æ—¶ä¼ å…¥ default
    return st.multiselect(label, options, default=init_value, key=key)


# ==================== æ•°æ®è·å–å‡½æ•°ï¼ˆå¸¦ç‰ˆæœ¬æ§åˆ¶ï¼‰ ====================
@st.cache_data(ttl=3600)
def get_high_level_data(_data_version):
    """
    æ·»åŠ æ•°æ®ç‰ˆæœ¬å‚æ•°ï¼Œå½“ç‰ˆæœ¬å˜åŒ–æ—¶ç¼“å­˜è‡ªåŠ¨å¤±æ•ˆ
    æ³¨æ„ï¼šæ­¤å¤„ä»£ç ä¿æŒä¸å˜ï¼Œåªä¿®æ”¹æ§ä»¶å¸ƒå±€éƒ¨åˆ†
    """
    # ... ä¿æŒåŸæœ‰ä»£ç ä¸å˜ ...
    db = get_db()

    # ä¿®æ­£çš„ SQL æŸ¥è¯¢ï¼šå¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„Taxæ•°æ®
    daily_sql = """
    WITH transaction_totals AS (
        SELECT 
            date(Datetime) AS date,
            [Transaction ID] AS txn_id,
            -- è®¡ç®—æ¯ä¸ª Transaction çš„æ€» Gross Sales å’Œæ€» Tax
            SUM([Gross Sales]) AS total_gross_sales,
            -- å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„Taxæ•°æ®ï¼šç§»é™¤$ç¬¦å·å¹¶è½¬æ¢ä¸ºæ•°å­—
            SUM(COALESCE(CAST(REPLACE(REPLACE([Tax], '$', ''), ',', '') AS REAL), 0)) AS total_tax,
            SUM(Qty) AS total_qty
        FROM transactions
        GROUP BY date, [Transaction ID]
    )
    SELECT
        date,
        -- æŒ‰ Transaction å»é‡åçš„æ€»å’Œ (Gross Sales - Tax)
        SUM(total_gross_sales - total_tax) AS net_sales_with_tax,
        SUM(total_gross_sales) AS gross_sales,
        SUM(total_tax) AS total_tax,
        COUNT(DISTINCT txn_id) AS transactions,
        CASE 
            WHEN COUNT(DISTINCT txn_id) > 0 
            THEN SUM(total_gross_sales - total_tax) * 1.0 / COUNT(DISTINCT txn_id)
            ELSE 0 
        END AS avg_txn,
        SUM(total_qty) AS qty
    FROM transaction_totals
    GROUP BY date
    ORDER BY date;
    """

    category_sql = """
    WITH category_transactions AS (
        SELECT 
            date(Datetime) AS date,
            Category,
            [Transaction ID] AS txn_id,
            -- æŒ‰ Transaction + Category èšåˆ
            SUM([Net Sales]) AS cat_net_sales,
            -- å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„Taxæ•°æ®
            SUM(COALESCE(CAST(REPLACE(REPLACE([Tax], '$', ''), ',', '') AS REAL), 0)) AS cat_tax,
            SUM([Gross Sales]) AS cat_gross,
            SUM(Qty) AS cat_qty
        FROM transactions
        GROUP BY date, Category, [Transaction ID]
    ),
    category_daily AS (
        SELECT
            date,
            Category,
            txn_id,
            -- æ¯ä¸ª Transaction åœ¨è¯¥ç±»åˆ«ä¸‹çš„æ€»é¢ (Net Sales + Tax) - ä¿æŒbarç±»çš„åŸå§‹é€»è¾‘
            SUM(cat_net_sales + cat_tax) AS cat_total_with_tax,
            SUM(cat_net_sales) AS cat_net_sales,
            SUM(cat_tax) AS cat_tax,
            SUM(cat_gross) AS cat_gross,
            SUM(cat_qty) AS cat_qty
        FROM category_transactions
        GROUP BY date, Category, txn_id
    )
    SELECT
        date,
        Category,
        SUM(cat_total_with_tax) AS net_sales_with_tax,
        SUM(cat_net_sales) AS net_sales,
        SUM(cat_tax) AS total_tax,
        COUNT(DISTINCT txn_id) AS transactions,
        CASE 
            WHEN COUNT(DISTINCT txn_id) > 0 
            THEN SUM(cat_total_with_tax) * 1.0 / COUNT(DISTINCT txn_id)
            ELSE 0 
        END AS avg_txn,
        SUM(cat_gross) AS gross,
        SUM(cat_qty) AS qty
    FROM category_daily
    GROUP BY date, Category
    ORDER BY date, Category;
    """

    daily = pd.read_sql(daily_sql, db)
    category = pd.read_sql(category_sql, db)

    if not daily.empty:
        daily["date"] = pd.to_datetime(daily["date"])

    if not category.empty:
        category["date"] = pd.to_datetime(category["date"])

    return daily, category


@st.cache_data(ttl=3600)
def _prepare_inventory_grouped(inv: pd.DataFrame, _data_version):
    # ... ä¿æŒåŸæœ‰ä»£ç ä¸å˜ ...
    if inv is None or inv.empty:
        return pd.DataFrame(), None

    df = inv.copy()

    if "source_date" in df.columns:
        df["date"] = pd.to_datetime(df["source_date"], errors="coerce")
    else:
        return pd.DataFrame(), None

    # Category åˆ—
    if "Categories" in df.columns:
        df["Category"] = df["Categories"].astype(str)
    elif "Category" in df.columns:
        df["Category"] = df["Category"].astype(str)
    else:
        df["Category"] = "Unknown"

    # === ç”¨ catalogue ç°ç®— ===
    df["Quantity"] = pd.to_numeric(df.get("Current Quantity Vie Market & Bar", 0), errors="coerce").fillna(0).abs()
    df["Price"] = pd.to_numeric(df.get("Price", 0), errors="coerce").fillna(0)
    df["UnitCost"] = pd.to_numeric(df.get("Default Unit Cost", 0), errors="coerce").fillna(0)

    def calc_retail(row):
        O, AA, tax = row["Price"], row["Quantity"], str(row.get("Tax - GST (10%)", "")).strip().upper()
        return (O / 11 * 10) * AA if tax == "Y" else O * AA

    df["Retail Total"] = df.apply(calc_retail, axis=1)
    df["Inventory Value"] = df["UnitCost"] * df["Quantity"]
    df["Profit"] = df["Retail Total"] - df["Inventory Value"]

    # èšåˆ
    g = (
        df.groupby(["date", "Category"], as_index=False)[["Inventory Value", "Profit"]]
        .sum(min_count=1)
    )

    latest_date = g["date"].max() if not g.empty else None
    return g, latest_date


# ==================== è®¡ç®—ç¼“å­˜å‡½æ•°ï¼ˆå¸¦ç‰ˆæœ¬æ§åˆ¶ï¼‰ ====================
@st.cache_data(ttl=600)
def compute_filtered_data(time_range, data_sel, cats_sel, daily, category_tx, inv_grouped, today, _data_version):
    """ç¼“å­˜è¿‡æ»¤å’Œè®¡ç®—çš„ç»“æœ"""
    # ... ä¿æŒåŸæœ‰ä»£ç ä¸å˜ ...
    # é¦–å…ˆè·å–æ—¶é—´ç­›é€‰åçš„dailyæ•°æ®
    daily_filtered = daily.copy()

    # åº”ç”¨æ—¶é—´èŒƒå›´ç­›é€‰åˆ°dailyæ•°æ®
    if "WTD" in time_range:
        daily_filtered = daily_filtered[daily_filtered["date"] >= today - pd.Timedelta(days=7)]
    if "MTD" in time_range:
        daily_filtered = daily_filtered[daily_filtered["date"] >= today - pd.Timedelta(days=30)]
    if "YTD" in time_range:
        daily_filtered = daily_filtered[daily_filtered["date"] >= today - pd.Timedelta(days=365)]

    grouped_tx = category_tx.copy()

    # åº”ç”¨ç›¸åŒçš„æ—¶é—´èŒƒå›´ç­›é€‰åˆ°grouped_tx
    if "WTD" in time_range:
        grouped_tx = grouped_tx[grouped_tx["date"] >= today - pd.Timedelta(days=7)]
    if "MTD" in time_range:
        grouped_tx = grouped_tx[grouped_tx["date"] >= today - pd.Timedelta(days=30)]
    if "YTD" in time_range:
        grouped_tx = grouped_tx[grouped_tx["date"] >= today - pd.Timedelta(days=365)]

    grouped_inv = inv_grouped.copy()
    if not grouped_inv.empty:
        if "WTD" in time_range:
            grouped_inv = grouped_inv[grouped_inv["date"] >= today - pd.Timedelta(days=7)]
        if "MTD" in time_range:
            grouped_inv = grouped_inv[grouped_inv["date"] >= today - pd.Timedelta(days=30)]
        if "YTD" in time_range:
            grouped_inv = grouped_inv[grouped_inv["date"] >= today - pd.Timedelta(days=365)]

    bar_cats = {"Cafe Drinks", "Smoothie Bar", "Soups", "Sweet Treats", "Wraps & Salads"}
    small_cats = [c for c in cats_sel if c not in ("bar", "retail", "total")]
    parts_tx = []

    if small_cats:
        parts_tx.append(grouped_tx[grouped_tx["Category"].isin(small_cats)])

    # è®¡ç®—baræ€»é¢
    bar_categories_list = list(bar_cats)
    bar_df = grouped_tx[grouped_tx["Category"].isin(bar_categories_list)].copy()

    bar_agg = pd.DataFrame()
    if not bar_df.empty:
        # æŒ‰æ—¥æœŸèšåˆbaræ•°æ®
        bar_agg = (bar_df.groupby("date", as_index=False)
                   .agg(net_sales_with_tax=("net_sales_with_tax", "sum"),
                        net_sales=("net_sales", "sum"),
                        total_tax=("total_tax", "sum"),
                        transactions=("transactions", "sum"),
                        gross=("gross", "sum"),
                        qty=("qty", "sum")))
        bar_agg["avg_txn"] = (bar_agg["net_sales_with_tax"] / bar_agg["transactions"]).replace(
            [pd.NA, float("inf")], 0)
        bar_agg["Category"] = "bar"

        if "bar" in cats_sel:
            parts_tx.append(bar_agg)

    # è®¡ç®—retail
    if "retail" in cats_sel:
        bar_daily_totals = pd.DataFrame()
        if not bar_agg.empty:
            bar_daily_totals = bar_agg[["date", "net_sales_with_tax"]].rename(
                columns={"net_sales_with_tax": "bar_total"})

        retail_data = []
        for date_val in daily_filtered["date"].unique():
            date_total = daily_filtered[daily_filtered["date"] == date_val]["net_sales_with_tax"].sum()

            bar_total = 0
            if not bar_daily_totals.empty and date_val in bar_daily_totals["date"].values:
                bar_total = bar_daily_totals[bar_daily_totals["date"] == date_val]["bar_total"].iloc[0]

            retail_total = proper_round(date_total - bar_total)

            date_transactions = daily_filtered[daily_filtered["date"] == date_val]["transactions"].sum()
            date_qty = daily_filtered[daily_filtered["date"] == date_val]["qty"].sum()

            retail_data.append({
                "date": date_val,
                "net_sales_with_tax": retail_total,
                "net_sales": retail_total,
                "total_tax": 0,
                "transactions": date_transactions,
                "avg_txn": retail_total / date_transactions if date_transactions > 0 else 0,
                "gross": 0,
                "qty": date_qty,
                "Category": "retail"
            })

        if retail_data:
            retail_agg = pd.DataFrame(retail_data)
            parts_tx.append(retail_agg)

    # è®¡ç®—total
    if "total" in cats_sel:
        total_data = []
        for date_val in daily_filtered["date"].unique():
            date_total = daily_filtered[daily_filtered["date"] == date_val]["net_sales_with_tax"].sum()

            date_transactions = daily_filtered[daily_filtered["date"] == date_val]["transactions"].sum()
            date_qty = daily_filtered[daily_filtered["date"] == date_val]["qty"].sum()

            total_data.append({
                "date": date_val,
                "net_sales_with_tax": date_total,
                "net_sales": date_total,
                "total_tax": daily_filtered[daily_filtered["date"] == date_val]["total_tax"].sum(),
                "transactions": date_transactions,
                "avg_txn": date_total / date_transactions if date_transactions > 0 else 0,
                "gross": daily_filtered[daily_filtered["date"] == date_val]["gross_sales"].sum(),
                "qty": date_qty,
                "Category": "total"
            })

        if total_data:
            total_agg = pd.DataFrame(total_data)
            parts_tx.append(total_agg)

    # åˆå¹¶æ‰€æœ‰æ•°æ®
    if parts_tx:
        grouped_tx = pd.concat(parts_tx, ignore_index=True)
        grouped_tx = grouped_tx.sort_values(["Category", "date"])
    else:
        grouped_tx = grouped_tx.iloc[0:0]

    parts_inv = []
    if not grouped_inv.empty:
        if small_cats:
            parts_inv.append(grouped_inv[grouped_inv["Category"].isin(small_cats)])

        if "bar" in cats_sel:
            bar_inv = grouped_inv[grouped_inv["Category"].isin(list(bar_cats))]
            if not bar_inv.empty:
                agg = (bar_inv.groupby("date", as_index=False)
                       .agg(**{"Inventory Value": ("Inventory Value", "sum"),
                               "Profit": ("Profit", "sum")}))
                agg["Category"] = "bar"
                parts_inv.append(agg)

        if "retail" in cats_sel:
            retail_inv = grouped_inv[~grouped_inv["Category"].isin(list(bar_cats))]
            if not retail_inv.empty:
                agg = (retail_inv.groupby("date", as_index=False)
                       .agg(**{"Inventory Value": ("Inventory Value", "sum"),
                               "Profit": ("Profit", "sum")}))
                agg["Category"] = "retail"
                parts_inv.append(agg)

        if "total" in cats_sel:
            total_inv = grouped_inv.copy()
            if not total_inv.empty:
                agg = (total_inv.groupby("date", as_index=False)
                       .agg(**{"Inventory Value": ("Inventory Value", "sum"),
                               "Profit": ("Profit", "sum")}))
                agg["Category"] = "total"
                parts_inv.append(agg)

    grouped_inv = pd.concat(parts_inv, ignore_index=True) if parts_inv else grouped_inv.iloc[0:0]

    return grouped_tx, grouped_inv


# ==================== ä¼˜åŒ–çš„æ¨¡å—åŒ–å‡½æ•° ====================

def render_cache_control():
    """æ¸²æŸ“ç¼“å­˜æ§åˆ¶ç»„ä»¶"""
    with st.sidebar.expander("ğŸ”„ Cache Control"):
        if st.button("ğŸ”„ Refresh Data Cache", type="primary", use_container_width=True):
            clear_all_cache()
            st.success("âœ… Data cache refreshed! New data will be loaded.")
            st.rerun()


def render_kpi_section(daily, tx, selected_date, inv_grouped, inv_latest_date):
    """æ¸²æŸ“KPIæŒ‡æ ‡éƒ¨åˆ†"""
    st.markdown(f"### ğŸ“… Selected Date: {selected_date}")

    # è®¡ç®—å®¢æˆ·æ•°é‡
    def calculate_customer_count(tx_df, selected_date):
        if tx_df is None or tx_df.empty:
            return 0

        if 'Datetime' not in tx_df.columns:
            return 0

        tx_df = tx_df.copy()
        tx_df['Datetime'] = pd.to_datetime(tx_df['Datetime'], errors='coerce')
        tx_df = tx_df.dropna(subset=['Datetime'])

        if tx_df.empty:
            return 0

        selected_date_str = selected_date.strftime('%Y-%m-%d')
        daily_tx = tx_df[tx_df['Datetime'].dt.strftime('%Y-%m-%d') == selected_date_str]

        if daily_tx.empty:
            return 0

        if 'Card Brand' not in daily_tx.columns or 'PAN Suffix' not in daily_tx.columns:
            return 0

        filtered_tx = daily_tx.dropna(subset=['Card Brand', 'PAN Suffix'])
        if filtered_tx.empty:
            return 0

        filtered_tx['Card Brand'] = filtered_tx['Card Brand'].str.title()
        filtered_tx['PAN Suffix'] = filtered_tx['PAN Suffix'].astype(str).str.split('.').str[0]
        unique_customers = filtered_tx[['Card Brand', 'PAN Suffix']].drop_duplicates()

        return len(unique_customers)

    selected_date_ts = pd.Timestamp(selected_date)
    df_selected_date = daily[daily["date"] == selected_date_ts]

    # è®¡ç®—KPIæŒ‡æ ‡
    kpis_main = {
        "Daily Net Sales": proper_round(df_selected_date["net_sales_with_tax"].sum()),
        "Daily Transactions": df_selected_date["transactions"].sum(),
        "Number of Customers": calculate_customer_count(tx, selected_date),
        "Avg Transaction": df_selected_date["avg_txn"].mean(),
        "3M Avg": proper_round(daily["net_sales_with_tax"].rolling(90, min_periods=1).mean().iloc[-1]),
        "6M Avg": proper_round(daily["net_sales_with_tax"].rolling(180, min_periods=1).mean().iloc[-1]),
        "Items Sold": df_selected_date["qty"].sum(),
    }

    inv_value_latest = 0.0
    profit_latest = 0.0
    if inv_grouped is not None and not inv_grouped.empty and inv_latest_date is not None:
        sub = inv_grouped[inv_grouped["date"] == inv_latest_date]
        inv_value_latest = float(pd.to_numeric(sub["Inventory Value"], errors="coerce").sum())
        profit_latest = float(pd.to_numeric(sub["Profit"], errors="coerce").sum())

    labels_values = list(kpis_main.items()) + [
        ("Inventory Value", inv_value_latest),
        ("Profit (Amount)", profit_latest),
    ]
    captions = {
        "Inventory Value": f"as of {pd.to_datetime(inv_latest_date).strftime('%Y-%m-%d') if inv_latest_date else '-'}",
        "Profit (Amount)": f"as of {pd.to_datetime(inv_latest_date).strftime('%Y-%m-%d') if inv_latest_date else '-'}",
    }

    for row in range(0, len(labels_values), 4):
        cols = st.columns(4)
        for i, col in enumerate(cols):
            idx = row + i
            if idx < len(labels_values):
                label, val = labels_values[idx]
                if pd.isna(val):
                    display = "-"
                else:
                    if label == "Avg Transaction":
                        display = f"{val:,.2f}"
                    elif label in ["Daily Transactions", "Items Sold", "Number of Customers"]:
                        display = f"{int(proper_round(val)):,}"
                    else:
                        display = f"${proper_round(val):,}"
                with col:
                    st.markdown(
                        f"<div style='font-size:28px; font-weight:600'>{display}</div>",
                        unsafe_allow_html=True
                    )
                    st.caption(label)
                    if label in captions:
                        st.caption(captions[label])


def render_interactive_section(daily, category_tx, inv_grouped, data_version):
    """æ¸²æŸ“äº¤äº’å¼å›¾è¡¨å’Œè¿‡æ»¤éƒ¨åˆ†"""
    st.subheader("ğŸ” Select Parameters")

    # ä½¿ç”¨ç´§å‡‘çš„åˆ—å¸ƒå±€ï¼Œè®©æ§ä»¶æ›´çŸ­
    col1, col2, col3 = st.columns([1, 1, 1])

    with col1:
        time_range_options = ["Custom dates", "WTD", "MTD", "YTD"]
        time_range = persisting_multiselect(
            "Choose time range",
            time_range_options,
            "hl_time"
        )

    with col2:
        data_options = [
            "Daily Net Sales", "Daily Transactions", "Avg Transaction", "3M Avg", "6M Avg",
            "Inventory Value", "Profit (Amount)", "Items Sold"
        ]
        data_sel = persisting_multiselect(
            "Choose data type",
            data_options,
            "hl_data"
        )

    with col3:
        bar_cats = {"Cafe Drinks", "Smoothie Bar", "Soups", "Sweet Treats", "Wraps & Salads"}
        all_cats_tx = sorted(category_tx["Category"].fillna("Unknown").unique().tolist())
        special_cats = ["bar", "retail", "total"]
        all_cats_extended = special_cats + sorted([c for c in all_cats_tx if c not in special_cats])
        cats_sel = persisting_multiselect(
            "Choose categories",
            all_cats_extended,
            "hl_cats"
        )

    custom_dates_selected = False
    t1 = None
    t2 = None

    # è‡ªå®šä¹‰æ—¥æœŸé€‰æ‹©å™¨ä½¿ç”¨ä¸ä¸Šé¢ç›¸åŒçš„ä¸‰åˆ—å¸ƒå±€
    if "Custom dates" in time_range:
        custom_dates_selected = True
        # ä½¿ç”¨ç›¸åŒçš„ä¸‰åˆ—å¸ƒå±€æ¥ä¿æŒå®½åº¦ä¸€è‡´
        date_col1, date_col2, date_col3 = st.columns([1, 1, 1])

        with date_col1:
            st.markdown("**From:**")
            t1 = st.date_input(
                "From Date",
                value=pd.Timestamp.today().normalize() - pd.Timedelta(days=7),
                key="date_from",
                label_visibility="collapsed"
            )

        with date_col2:
            st.markdown("**To:**")
            t2 = st.date_input(
                "To Date",
                value=pd.Timestamp.today().normalize(),
                key="date_to",
                label_visibility="collapsed"
            )

        # ç¬¬ä¸‰åˆ—ç•™ç©ºä»¥ä¿æŒå¸ƒå±€å¯¹é½
        with date_col3:
            st.write("")  # ç©ºåˆ—ç”¨äºå¯¹é½

    # åˆ›å»ºè¿‡æ»¤å‚æ•°çš„å“ˆå¸Œé”®
    filter_params = {
        'time_range': tuple(time_range) if time_range else (),
        'data_sel': tuple(data_sel) if data_sel else (),
        'cats_sel': tuple(cats_sel) if cats_sel else (),
        't1': t1,
        't2': t2,
        'data_version': data_version
    }
    filter_hash = hashlib.md5(str(filter_params).encode()).hexdigest()
    cache_key_filtered = f'cached_filtered_data_{filter_hash}'

    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è®¡ç®—
    current_filter_state = {
        'time_range': time_range,
        'data_sel': data_sel,
        'cats_sel': cats_sel,
        't1': t1,
        't2': t2
    }

    # è·å–ä¸Šä¸€æ¬¡çš„è¿‡æ»¤çŠ¶æ€
    last_filter_state = st.session_state.get('last_filter_state', {})

    # åªæœ‰å½“è¿‡æ»¤æ¡ä»¶ç¡®å®å‘ç”Ÿå˜åŒ–æ—¶æ‰é‡æ–°è®¡ç®—
    filter_changed = current_filter_state != last_filter_state

    if time_range and data_sel and cats_sel:
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„è¿‡æ»¤ç»“æœï¼Œæˆ–è€…è¿‡æ»¤æ¡ä»¶æ²¡æœ‰å˜åŒ–
        if cache_key_filtered in st.session_state and not filter_changed:
            grouped_tx, grouped_inv = st.session_state[cache_key_filtered]
        else:
            # åªæœ‰å½“è¿‡æ»¤æ¡ä»¶å˜åŒ–æ—¶æ‰é‡æ–°è®¡ç®—
            if filter_changed:
                with st.spinner("ğŸ”„ Processing data..."):
                    today = pd.Timestamp.today().normalize()
                    grouped_tx, grouped_inv = compute_filtered_data(
                        time_range, data_sel, cats_sel, daily, category_tx, inv_grouped, today, data_version
                    )

                    # å¤„ç†è‡ªå®šä¹‰æ—¥æœŸèŒƒå›´
                    if custom_dates_selected and t1 and t2:
                        grouped_tx = grouped_tx[
                            (grouped_tx["date"] >= pd.to_datetime(t1)) & (grouped_tx["date"] <= pd.to_datetime(t2))]
                        if not grouped_inv.empty:
                            grouped_inv = grouped_inv[
                                (grouped_inv["date"] >= pd.to_datetime(t1)) & (
                                        grouped_inv["date"] <= pd.to_datetime(t2))]

                    # ç¼“å­˜ç»“æœ
                    st.session_state[cache_key_filtered] = (grouped_tx, grouped_inv)
                    # æ›´æ–°ä¸Šä¸€æ¬¡çš„è¿‡æ»¤çŠ¶æ€
                    st.session_state['last_filter_state'] = current_filter_state
            else:
                # ä½¿ç”¨ç°æœ‰çš„ç¼“å­˜æ•°æ®
                grouped_tx, grouped_inv = st.session_state[cache_key_filtered]

        mapping_tx = {
            "Daily Net Sales": ("net_sales_with_tax", "Daily Net Sales"),
            "Daily Transactions": ("transactions", "Daily Transactions"),
            "Avg Transaction": ("avg_txn", "Avg Transaction"),
            "3M Avg": ("net_sales_with_tax", "3M Avg (Rolling 90d)"),
            "6M Avg": ("net_sales_with_tax", "6M Avg (Rolling 180d)"),
            "Items Sold": ("qty", "Items Sold"),
        }
        mapping_inv = {
            "Inventory Value": ("Inventory Value", "Inventory Value"),
            "Profit (Amount)": ("Profit", "Profit (Retail - Inventory)"),
        }

        for metric in data_sel:
            if metric in mapping_tx:
                y, title = mapping_tx[metric]
                plot_df = grouped_tx.dropna(subset=[y]).copy()
                if metric in ["3M Avg", "6M Avg"]:
                    if metric == "3M Avg":
                        plot_df["rolling"] = plot_df.groupby("Category")[y].transform(
                            lambda x: x.rolling(90, min_periods=1).mean())
                    else:
                        plot_df["rolling"] = plot_df.groupby("Category")[y].transform(
                            lambda x: x.rolling(180, min_periods=1).mean())
                    fig = px.line(plot_df, x="date", y="rolling", color="Category", title=title, markers=True)
                else:
                    fig = px.line(plot_df, x="date", y=y, color="Category", title=title, markers=True)
                fig.update_layout(xaxis=dict(type="date"))
                st.plotly_chart(fig, use_container_width=True)

            elif metric in mapping_inv:
                y, title = mapping_inv[metric]
                if grouped_inv is not None and not grouped_inv.empty:
                    plot_df = grouped_inv.dropna(subset=[y]).copy()
                    fig = px.line(plot_df, x="date", y=y, color="Category", title=title, markers=True)
                    fig.update_layout(xaxis=dict(type="date"))
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.info(f"No inventory data to plot for {metric}.")

        st.subheader("ğŸ“‹ Detailed Data")
        tables = []

        if not grouped_tx.empty:
            cols_tx = ["date", "Category"]
            for sel in data_sel:
                if sel in mapping_tx:
                    cols_tx.append(mapping_tx[sel][0])
            table_tx = grouped_tx[cols_tx].copy()
            for col in table_tx.columns:
                if col in ["net_sales_with_tax", "avg_txn", "net_sales"]:
                    table_tx[f"{col}_raw"] = table_tx[col]
                    table_tx[col] = table_tx[col].apply(lambda x: proper_round(x) if pd.notna(x) else x)
                elif col in ["transactions", "qty"]:
                    table_tx[col] = table_tx[col].apply(lambda x: proper_round(x) if pd.notna(x) else x)
            table_tx["date"] = table_tx["date"].dt.strftime("%Y-%m-%d")
            table_tx = table_tx.sort_values(["Category", "date"])
            tables.append(
                table_tx.drop(columns=[col for col in table_tx.columns if col.endswith('_raw')], errors='ignore'))

        if not grouped_inv.empty:
            cols_inv = ["date", "Category"]
            for sel in data_sel:
                if sel in mapping_inv:
                    cols_inv.append(mapping_inv[sel][0])
            table_inv = grouped_inv[cols_inv].copy()
            for col in table_inv.columns:
                if col in ["Inventory Value", "Profit"]:
                    table_inv[col] = table_inv[col].apply(lambda x: proper_round(x) if pd.notna(x) else x)
            table_inv["date"] = table_inv["date"].dt.strftime("%Y-%m-%d")
            table_inv = table_inv.sort_values(["Category", "date"])
            tables.append(table_inv)

        if tables:
            out = pd.concat(tables, ignore_index=True)
            st.dataframe(out, use_container_width=True)
        else:
            st.info("No data for the selected filters.")
    else:
        st.info("Please select time range, data, and category to generate the chart.")

# ==================== ä¼˜åŒ–çš„ä¸»å‡½æ•° ====================
def show_high_level(tx: pd.DataFrame, mem: pd.DataFrame, inv: pd.DataFrame, data_updated=False):
    st.header("ğŸ“Š High Level Report")

    # æ¸²æŸ“ç¼“å­˜æ§åˆ¶
    render_cache_control()

    # æ£€æµ‹æ•°æ®å˜åŒ–
    current_data_hash = get_data_hash(tx, mem, inv)
    last_data_hash = st.session_state.get('last_data_hash')

    # è·å–æ•°æ®ç‰ˆæœ¬
    data_version = get_data_version()

    # å¦‚æœæ•°æ®å‘ç”Ÿå˜åŒ–æˆ–è€…è¿˜æ²¡æœ‰åŠ è½½æ•°æ®ï¼Œé‡æ–°è·å–æ•°æ®
    if (current_data_hash != last_data_hash or
            'precomputed_data' not in st.session_state or
            data_updated):

        with st.spinner("ğŸ”„ Loading data..."):
            daily, category_tx = get_high_level_data(data_version)
            inv_grouped, inv_latest_date = _prepare_inventory_grouped(inv, data_version)

            # ç¼“å­˜é¢„å¤„ç†çš„æ•°æ®
            st.session_state.precomputed_data = {
                'daily': daily,
                'category_tx': category_tx,
                'inv_grouped': inv_grouped,
                'inv_latest_date': inv_latest_date
            }

            # æ›´æ–°æ•°æ®å“ˆå¸Œ
            st.session_state.last_data_hash = current_data_hash
    else:
        # ä½¿ç”¨ç¼“å­˜çš„æ•°æ®
        precomputed = st.session_state.precomputed_data
        daily = precomputed['daily']
        category_tx = precomputed['category_tx']
        inv_grouped = precomputed['inv_grouped']
        inv_latest_date = precomputed['inv_latest_date']

    # æ—¥æœŸé€‰æ‹©å™¨ - ä½¿ç”¨ç´§å‡‘å¸ƒå±€
    date_col1, date_col2 = st.columns([1, 3])
    with date_col1:
        selected_date = st.date_input(
            "Select Date",
            value=min(pd.Timestamp.today().normalize().date(), daily["date"].max().date()),
            min_value=daily["date"].min().date(),
            max_value=pd.Timestamp.today().normalize().date(),
            key="date_selector"
        )

    # æ¸²æŸ“KPIéƒ¨åˆ†
    render_kpi_section(daily, tx, selected_date, inv_grouped, inv_latest_date)

    # æ¸²æŸ“äº¤äº’å¼éƒ¨åˆ†
    render_interactive_section(daily, category_tx, inv_grouped, data_version)