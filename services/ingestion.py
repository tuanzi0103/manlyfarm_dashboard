# services/ingestion.py
import os
import re
import tempfile
from io import BytesIO
from datetime import datetime, timedelta

import pandas as pd
import streamlit as st

from services.db import get_db

# === Google Drive Áõ∏ÂÖ≥ ===
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive

FOLDER_ID = "1lZGE0DkgKyox1HbBzuhZ-oypDF478jBj"

# ‚úÖ ÂÖ®Â±ÄÁºìÂ≠ò drive ÂÆû‰æã
_drive_instance = None


def get_drive():
    global _drive_instance
    if _drive_instance is not None:
        return _drive_instance

    gauth = GoogleAuth()

    # ‚úÖ ËÆæÁΩÆ OAuth ‰∏∫ offlineÔºåÁ°Æ‰øù refresh_token Â≠ò‰∏ãÊù•
    gauth.settings['oauth_scope'] = [
        "https://www.googleapis.com/auth/drive",
        "https://www.googleapis.com/auth/drive.file",
        "https://www.googleapis.com/auth/drive.metadata"
    ]
    gauth.settings['get_refresh_token'] = True

    token_path = "credentials.json"
    if os.path.exists(token_path):
        gauth.LoadCredentialsFile(token_path)

    if gauth.credentials is None:
        # Á¨¨‰∏ÄÊ¨°ËÆ§ËØÅÔºöÂä† offline
        gauth.LocalWebserverAuth()  # ËøôÈáå‰ºöÂºπÊµèËßàÂô®
    elif gauth.access_token_expired:
        gauth.Refresh()
    else:
        gauth.Authorize()

    gauth.SaveCredentialsFile(token_path)
    _drive_instance = GoogleDrive(gauth)
    return _drive_instance


def upload_file_to_drive(local_path: str, remote_name: str):
    """ÊääÊú¨Âú∞Êñá‰ª∂‰∏ä‰º†Âà∞ÊåáÂÆöÁöÑ Google Drive Êñá‰ª∂Â§π„ÄÇ"""
    try:
        drive = get_drive()
        f = drive.CreateFile({'title': remote_name, 'parents': [{'id': FOLDER_ID}]})
        f.SetContentFile(local_path)
        f.Upload()
    except Exception as e:
        # ‰∏ä‰º†Â§±Ë¥•‰∏çÂΩ±Âìç‰∏ªÊµÅÁ®ã
        st.sidebar.warning(f"‚ö†Ô∏è Upload to Drive failed: {e}")


def download_file_from_drive(file_id, local_path):
    drive = get_drive()
    f = drive.CreateFile({'id': file_id})
    f.GetContentFile(local_path)


# --------------- Â∑•ÂÖ∑ÂáΩÊï∞ ---------------

def _fix_header(df: pd.DataFrame) -> pd.DataFrame:
    """Ëã•Á¨¨‰∏ÄË°åÊòØ UnnamedÔºåÂ§öÊï∞ÊòØÂ§öË°åË°®Â§¥ÔºõÊääÁ¨¨‰∫åË°åÊèê‰∏∫Ë°®Â§¥„ÄÇ"""
    if len(df.columns) and all(str(c).startswith("Unnamed") for c in df.columns):
        df.columns = df.iloc[0]
        df = df.drop(index=0).reset_index(drop=True)
    return df


def _to_float(series: pd.Series) -> pd.Series:
    return (
        series.astype(str)
        .str.replace(r"[^0-9\.\-]", "", regex=True)
        .replace("", pd.NA)
        .astype(float)
    )


def _extract_date_from_filename(name: str):
    """‰ªéÊñá‰ª∂Âêç‰∏≠ÊèêÂèñ YYYY-MM-DD"""
    m = re.search(r"(\d{4}-\d{2}-\d{2})", name)
    if m:
        return m.group(1)
    return None


def _extract_date_range_from_filename(name: str):
    """‰ªéÊñá‰ª∂Âêç‰∏≠ÊèêÂèñÊó•ÊúüËåÉÂõ¥ÔºåËøîÂõû (start_date, end_date)"""
    # ÂåπÈÖçÊ†ºÂºèÂ¶Ç 2024-08-18_to_2024-09-18 Êàñ 2024-08-18-2024-09-18
    patterns = [
        r'(\d{4}-\d{2}-\d{2})_to_(\d{4}-\d{2}-\d{2})',
        r'(\d{4}-\d{2}-\d{2})-(\d{4}-\d{2}-\d{2})',
        r'(\d{4}\.\d{2}\.\d{2})_to_(\d{4}\.\d{2}\.\d{2})',
        r'(\d{4}\.\d{2}\.\d{2})-(\d{4}\.\d{2}\.\d{2})'
    ]

    for pattern in patterns:
        m = re.search(pattern, name)
        if m:
            try:
                start_date = pd.to_datetime(m.group(1)).date()
                end_date = pd.to_datetime(m.group(2)).date()
                return start_date, end_date
            except:
                continue

    # Â¶ÇÊûúÊâæ‰∏çÂà∞Êó•ÊúüËåÉÂõ¥ÔºåÂ∞ùËØï‰ªéÂçï‰∏™Êó•ÊúüÊé®Êñ≠
    single_date = _extract_date_from_filename(name)
    if single_date:
        date_obj = pd.to_datetime(single_date).date()
        return date_obj, date_obj

    return None, None


# --------------- È¢ÑÂ§ÑÁêÜÔºà‰∏çÊîπÂàóÂêçÔºâ ---------------

def preprocess_transactions(df: pd.DataFrame) -> pd.DataFrame:
    df = _fix_header(df)
    if "Date" in df.columns and "Time" in df.columns:
        df["Datetime"] = pd.to_datetime(
            df["Date"].astype(str) + " " + df["Time"].astype(str),
            errors="coerce"
        )
        drop_cols = [c for c in ["Date", "Time", "Time Zone"] if c in df.columns]
        df = df.drop(columns=drop_cols)
    elif "Datetime" in df.columns:
        df["Datetime"] = pd.to_datetime(df["Datetime"], errors="coerce")

    # Á°Æ‰øùÊúâÊó•ÊúüÂàóÁî®‰∫éÂêéÁª≠Â§ÑÁêÜ
    if "Datetime" in df.columns:
        df["date"] = pd.to_datetime(df["Datetime"], errors="coerce").dt.date

    for col in ["Net Sales", "Gross Sales", "Qty", "Discounts"]:
        if col in df.columns:
            df[col] = _to_float(df[col])

    if "Customer ID" not in df.columns:
        df["Customer ID"] = None

    return df


def preprocess_inventory(df: pd.DataFrame, filename: str = None) -> pd.DataFrame:
    df = _fix_header(df)
    required = [
        "Tax - GST (10%)", "Price", "Current Quantity Vie Market & Bar",
        "Default Unit Cost", "Categories"
    ]
    for col in required:
        if col not in df.columns:
            df[col] = None
    if filename:
        df["source_date"] = _extract_date_from_filename(filename)
    return df


def preprocess_members(df: pd.DataFrame) -> pd.DataFrame:
    return _fix_header(df)


# --------------- Ë°®ÁªìÊûÑÂØπÈΩê & ÂéªÈáç & ÂÜôÂÖ• ---------------

def _table_exists(conn, table: str) -> bool:
    try:
        cur = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name=?", (table,))
        return cur.fetchone() is not None
    except Exception:
        return False


def _existing_columns(conn, table: str) -> list:
    try:
        cur = conn.execute(f"PRAGMA table_info('{table}')")
        return [row[1] for row in cur.fetchall()]
    except Exception:
        return []


def _add_missing_columns(conn, table: str, missing_cols: list, prefer_real: set):
    cur = conn.cursor()
    for col in missing_cols:
        coltype = "REAL" if col in prefer_real else "TEXT"
        cur.execute(f'''ALTER TABLE "{table}" ADD COLUMN "{col}" {coltype}''')
    conn.commit()


def _ensure_table_schema(conn, table: str, df: pd.DataFrame, prefer_real: set):
    if not _table_exists(conn, table):
        df.head(0).to_sql(table, conn, if_exists="replace", index=False)
        return
    cols_now = set(_existing_columns(conn, table))
    incoming = list(df.columns)
    missing = [c for c in incoming if c not in cols_now]
    if missing:
        _add_missing_columns(conn, table, missing, prefer_real)


def _get_existing_date_ranges(conn, table: str):
    """Ëé∑ÂèñÊï∞ÊçÆÂ∫ì‰∏≠Â∑≤Â≠òÂú®ÁöÑÊó•ÊúüËåÉÂõ¥"""
    try:
        if table == "transactions":
            query = "SELECT MIN(date), MAX(date) FROM transactions WHERE date IS NOT NULL"
            result = conn.execute(query).fetchone()
            if result and result[0] and result[1]:
                return pd.to_datetime(result[0]).date(), pd.to_datetime(result[1]).date()
    except Exception as e:
        st.sidebar.warning(f"Warning getting date ranges: {e}")
    return None, None


def _delete_overlapping_dates(conn, table: str, start_date, end_date, filename: str):
    """Âà†Èô§‰∏éÊñ∞Êñá‰ª∂Êó•ÊúüËåÉÂõ¥ÈáçÂè†ÁöÑÁé∞ÊúâËÆ∞ÂΩï"""
    try:
        if table == "transactions" and start_date and end_date:
            # Âà†Èô§‰∏éÊñ∞Êñá‰ª∂Êó•ÊúüËåÉÂõ¥ÂÆåÂÖ®ÈáçÂè†ÁöÑÁé∞ÊúâËÆ∞ÂΩï
            delete_query = "DELETE FROM transactions WHERE date >= ? AND date <= ?"
            conn.execute(delete_query, (start_date, end_date))
            conn.commit()
            st.sidebar.info(f"üîÑ Replacing data for {start_date} to {end_date} from {filename}")
            return True
    except Exception as e:
        st.sidebar.warning(f"Warning deleting overlapping data: {e}")
    return False


def _smart_deduplicate_transactions(df: pd.DataFrame, conn, filename: str):
    """Êô∫ËÉΩÂéªÈáçÈÄªËæëÔºöÂü∫‰∫éÊó•ÊúüËåÉÂõ¥ÂíåTransaction ID"""
    if df.empty or "date" not in df.columns or "Transaction ID" not in df.columns:
        return df

    # ‰ªéÊñá‰ª∂ÂêçÊèêÂèñÊó•ÊúüËåÉÂõ¥
    file_start_date, file_end_date = _extract_date_range_from_filename(filename)

    if not file_start_date or not file_end_date:
        # Â¶ÇÊûúÊó†Ê≥ï‰ªéÊñá‰ª∂ÂêçÊèêÂèñÊó•ÊúüËåÉÂõ¥Ôºå‰ΩøÁî®Êï∞ÊçÆ‰∏≠ÁöÑÂÆûÈôÖÊó•ÊúüËåÉÂõ¥
        file_start_date = df["date"].min()
        file_end_date = df["date"].max()
        st.sidebar.info(f"üìÖ Using data date range: {file_start_date} to {file_end_date}")

    if file_start_date and file_end_date:
        # Âà†Èô§ËØ•Êó•ÊúüËåÉÂõ¥ÂÜÖÁöÑÁé∞ÊúâÊï∞ÊçÆ
        _delete_overlapping_dates(conn, "transactions", file_start_date, file_end_date, filename)

        # ËøîÂõûÊâÄÊúâÊï∞ÊçÆÔºàÂõ†‰∏∫Â∑≤ÁªèÂà†Èô§‰∫ÜÈáçÂè†ÈÉ®ÂàÜÔºâ
        return df
    else:
        # ÂõûÈÄÄÂà∞ÂéüÊù•ÁöÑÂéªÈáçÈÄªËæë
        st.sidebar.warning("‚ö†Ô∏è Could not determine date range, using fallback deduplication")
        return _deduplicate(df, ["Transaction ID", "date"], conn, "transactions")


def _deduplicate(df: pd.DataFrame, key_cols: list, conn, table: str) -> pd.DataFrame:
    """ÊîØÊåÅÂ§öÂàóËÅîÂêàÂéªÈáç"""
    try:
        cols = [c for c in key_cols if c in df.columns]
        if not cols:
            return df
        query_cols = ", ".join([f'"{c}"' for c in cols])
        exist = pd.read_sql(f"SELECT {query_cols} FROM {table}", conn)
        if exist.empty:
            return df
        merged = df.merge(exist, on=cols, how="left", indicator=True)
        return merged[merged["_merge"] == "left_only"].drop(columns="_merge")
    except Exception:
        return df


def _write_df(conn, df: pd.DataFrame, table: str, key_candidates: list, prefer_real: set, filename: str = None):
    _ensure_table_schema(conn, table, df, prefer_real)

    # === ÈíàÂØπ transactions ÁöÑÊô∫ËÉΩÂ§ÑÁêÜ ===
    if table == "transactions":
        # Á°Æ‰øùÊúâÊó•ÊúüÂàó
        if "Datetime" in df.columns and "date" not in df.columns:
            df["date"] = pd.to_datetime(df["Datetime"], errors="coerce").dt.date

        if "date" in df.columns and filename:
            # ‰ΩøÁî®Êô∫ËÉΩÂéªÈáçÈÄªËæë
            df = _smart_deduplicate_transactions(df, conn, filename)
        else:
            # ÂõûÈÄÄÂà∞ÂéüÊù•ÁöÑÂéªÈáçÈÄªËæë
            df = _deduplicate(df, ["Transaction ID", "date"], conn, table)

    # === ÈíàÂØπ inventory ===
    elif table == "inventory" and "source_date" in df.columns:
        dates = df["source_date"].dropna().unique().tolist()
        if dates:
            for d in dates:
                conn.execute(f'DELETE FROM "{table}" WHERE source_date=?', (d,))
            conn.commit()
        # ‚úÖ Êîπ‰∏∫ SKU + source_date ËÅîÂêàÂéªÈáç
        df = _deduplicate(df, ["SKU", "source_date"], conn, table)

    # === ÈªòËÆ§ÊÉÖÂÜµÔºàmembersÁ≠âÔºâ ===
    else:
        key_col = next((k for k in key_candidates if k in df.columns), None)
        if key_col:
            df = _deduplicate(df, [key_col], conn, table)

    if not df.empty:
        df.to_sql(table, conn, if_exists="append", index=False)
        return len(df)
    return 0


# --------------- Á¥¢Âºï ---------------
def ensure_indexes():
    conn = get_db()
    cur = conn.cursor()
    cur.execute('CREATE INDEX IF NOT EXISTS idx_txn_datetime ON transactions(Datetime)')
    cur.execute('CREATE INDEX IF NOT EXISTS idx_txn_id ON transactions([Transaction ID])')
    cur.execute('CREATE INDEX IF NOT EXISTS idx_txn_date ON transactions(date)')
    cur.execute('CREATE INDEX IF NOT EXISTS idx_member_square ON members([Square Customer ID])')
    cur.execute('CREATE INDEX IF NOT EXISTS idx_member_ref ON members([Reference ID])')
    cur.execute('CREATE INDEX IF NOT EXISTS idx_inv_sku ON inventory(SKU)')
    conn.commit()


# --------------- ‰ªé Google Drive ÂØºÂÖ• ---------------
def ingest_from_drive_all():
    conn = get_db()
    drive = get_drive()
    files = drive.ListFile({'q': f"'{FOLDER_ID}' in parents and trashed=false"}).GetList()
    if not files:
        return

    # ÊåâÊñá‰ª∂ÂêçÊéíÂ∫èÔºåÁ°Æ‰øùËæÉÊñ∞ÁöÑÊñá‰ª∂Âú®ÂêéÈù¢Â§ÑÁêÜÔºàÂ¶ÇÊûúÈúÄË¶ÅË¶ÜÁõñÈÄªËæëÔºâ
    files.sort(key=lambda x: x["title"])

    seen = set()
    total_imported = 0

    for f in files:
        name = f["title"]
        if name in seen:
            continue
        seen.add(name)

        local = os.path.join(tempfile.gettempdir(), name)
        f.GetContentFile(local)

        if name.lower().endswith(".csv"):
            df = pd.read_csv(local)
            df = _fix_header(df)
        elif name.lower().endswith(".xlsx"):
            header_row = 1 if "catalogue" in name.lower() else 0
            df = pd.read_excel(local, header=header_row)
            df = _fix_header(df)
        else:
            continue

        imported_count = 0
        if "items" in name.lower():
            df = preprocess_transactions(df)
            imported_count = _write_df(conn, df, "transactions",
                                       key_candidates=["Transaction ID", "Item", "Price", "Modifiers Applied"],
                                       prefer_real={"Net Sales", "Gross Sales", "Qty", "Discounts"},
                                       filename=name)
            st.sidebar.info(f"üìä Processed {name}: {imported_count} transactions")

        elif "catalogue" in name.lower():
            df = preprocess_inventory(df, filename=name)
            imported_count = _write_df(conn, df, "inventory",
                                       key_candidates=["SKU"], prefer_real=set(),
                                       filename=name)
            st.sidebar.info(f"üì¶ Processed {name}: {imported_count} inventory items")

        elif "member" in name.lower():
            df = preprocess_members(df)
            imported_count = _write_df(conn, df, "members",
                                       key_candidates=["Square Customer ID", "Reference ID"],
                                       prefer_real=set(), filename=name)
            st.sidebar.info(f"üë• Processed {name}: {imported_count} members")

        total_imported += imported_count

        try:
            os.remove(local)
        except Exception:
            pass

    ensure_indexes()
    if total_imported > 0:
        st.sidebar.success(f"‚úÖ Imported {total_imported} total records from Google Drive")


@st.cache_data(show_spinner=False)
def init_db_from_drive_once():
    try:
        ingest_from_drive_all()
    except Exception as e:
        st.sidebar.warning(f"‚ö†Ô∏è Auto-ingest from Drive failed: {e}")
    return True


# --------------- ÊâãÂä®ÂØºÂÖ•ÔºàSidebar ‰∏ä‰º†Ôºâ ---------------
def ingest_csv(uploaded_file):
    conn = get_db()
    filename = uploaded_file.name if hasattr(uploaded_file, "name") else "uploaded.csv"
    st.sidebar.info(f"üìÇ Importing {filename}")

    df = pd.read_csv(uploaded_file)
    df = _fix_header(df)

    imported_count = 0
    try:
        if "Net Sales" in df.columns and "Gross Sales" in df.columns:
            df = preprocess_transactions(df)
            imported_count = _write_df(conn, df, "transactions",
                                       key_candidates=["Transaction ID"],
                                       prefer_real={"Net Sales", "Gross Sales", "Qty", "Discounts"},
                                       filename=filename)
            st.sidebar.success(f"‚úÖ Imported {imported_count} rows (transactions)")

        elif "SKU" in df.columns or "Stock on Hand" in df.columns or "Categories" in df.columns:
            df = preprocess_inventory(df, filename=filename)
            imported_count = _write_df(conn, df, "inventory",
                                       key_candidates=["SKU"], prefer_real=set(),
                                       filename=filename)
            st.sidebar.success(f"‚úÖ Imported {imported_count} rows (inventory)")

        elif "Square Customer ID" in df.columns or "First Name" in df.columns or "member" in filename.lower():
            df = preprocess_members(df)
            imported_count = _write_df(conn, df, "members",
                                       key_candidates=["Square Customer ID", "Reference ID"],
                                       prefer_real=set(), filename=filename)
            st.sidebar.success(f"‚úÖ Imported {imported_count} rows (members)")

        else:
            st.sidebar.warning(f"‚ö†Ô∏è Skipped {filename}, schema not recognized")

        tmp_path = os.path.join(tempfile.gettempdir(), filename)
        with open(tmp_path, "wb") as f_local:
            if hasattr(uploaded_file, "getbuffer"):
                f_local.write(uploaded_file.getbuffer())
            else:
                f_local.write(uploaded_file.read())
        upload_file_to_drive(tmp_path, filename)

    finally:
        try:
            os.remove(tmp_path)
        except Exception:
            pass

    ensure_indexes()
    return True


def ingest_excel(uploaded_file):
    conn = get_db()
    filename = uploaded_file.name if hasattr(uploaded_file, "name") else "uploaded.xlsx"
    st.sidebar.info(f"üìÇ Importing {filename}")

    data = uploaded_file.read()
    xls = pd.ExcelFile(BytesIO(data))

    total_imported = 0
    try:
        for sheet in xls.sheet_names:
            header_row = 1 if ("catalogue" in filename.lower()) else 0
            df = pd.read_excel(xls, sheet_name=sheet, header=header_row)
            df = _fix_header(df)

            imported_count = 0
            if "Net Sales" in df.columns and "Gross Sales" in df.columns:
                df = preprocess_transactions(df)
                imported_count = _write_df(conn, df, "transactions",
                                           key_candidates=["Transaction ID"],
                                           prefer_real={"Net Sales", "Gross Sales", "Qty", "Discounts"},
                                           filename=filename)

            elif "SKU" in df.columns or "Stock on Hand" in df.columns or "Categories" in df.columns:
                df = preprocess_inventory(df, filename=filename)
                imported_count = _write_df(conn, df, "inventory",
                                           key_candidates=["SKU"], prefer_real=set(),
                                           filename=filename)

            elif "Square Customer ID" in df.columns or "First Name" in df.columns or "member" in filename.lower():
                df = preprocess_members(df)
                imported_count = _write_df(conn, df, "members",
                                           key_candidates=["Square Customer ID", "Reference ID"],
                                           prefer_real=set(), filename=filename)

            total_imported += imported_count

        st.sidebar.success(f"‚úÖ {filename} imported: {total_imported} total records")

        tmp_path = os.path.join(tempfile.gettempdir(), filename)
        with open(tmp_path, "wb") as f_local:
            f_local.write(data)
        upload_file_to_drive(tmp_path, filename)

    finally:
        try:
            os.remove(tmp_path)
        except Exception:
            pass

    ensure_indexes()
    return True


__all__ = [
    "ingest_csv",
    "ingest_excel",
    "ingest_from_drive_all",
    "get_drive",
    "upload_file_to_drive",
    "download_file_from_drive",
    "init_db_from_drive_once",
]